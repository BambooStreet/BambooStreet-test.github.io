---
title: "[HelloWorld 개발일지] Bllossom 모델 도입 필요성 검토를 위한, VLLM과 양자화 실험"
author: BambooStreet
date: 2024-09-26 11:31:00 +0800
categories: [study]
tags: []
image: assets/img/posts/20240925/thumbnail.png
---

## 실험 필요성?

기존 저희 챗봇 프로토타입은 GPT-api를 사용하고 있었는데, 향후 사용자 수가 증가함에 따라(과연 그럴까..) api 비용도 증가될까 우려되어, 자체 모델을 서빙해서 사용하는 방법도 고려하고 있었습니다. 이를 위해선 역시 비용 관점에서 하드웨어 요구 사항을 정확히 파악하고, 모델의 성능에 대한 실험을 통해 그 효용성을 계산할 필요가 있었습니다. 따라서 기존 프로토타입과 비교해 양자화와 vLLM을 사용한 모델이 얼만큼의 효용성을 보이는지 직접 비교, 테스트 하는 실험을 진행해 봤습니다.



## 실험

본 실험은 대규모 언어 모델(LLM)의 성능 최적화 기법인 양자화와 추론 가속화 라이브러리인 vLLM의 효과를 평가하기 위해 설계되었습니다. 구체적으로, 한국어 LLaMA 모델인 "llama-3-Korean-Bllossom-8B"를 대상으로 하여 양자화와 vLLM 적용이 모델의 성능과 효율성에 미치는 영향을 분석하고자 합니다.



### 실험 설계

1. 통제 변인:
   * 샘플링 파라미터: 모든 조건에서 동일한 샘플링 파라미터를 사용하여 텍스트 생성의 일관성을 유지
      - Temperature: 0.6
      - Top-p: 0.95
      - Max tokens: 2048
   * 프롬프트: 모든 조건에서 세 가지 동일한 프롬프트 사용
   * ```python
        "서울의 유명한 관광 코스를 만들어줄래?"
        "한국 전통 음식 중 외국인에게 추천할 만한 것은?"
        "기후 변화에 대응하기 위해 개인이 할 수 있는 일은?"
        ```

   * 시스템 프롬프트: 모든 조건에서 동일한 시스템 프롬프트 사용
   * ```python
      "You are a helpful AI assistant. Please answer the user's questions kindly. 당신은 유능한 AI 어시스턴트 입니다. 사용자의 질문에 대해 친절하게 답변해주세요."
        ```

2. 변인:
   * 양자화 여부:
      - 원본 모델: MLP-KTLim/llama-3-Korean-Bllossom-8B
      - 양자화 모델: ohmyhong/llama-3-Korean-Bllossom-8B-awq (AWQ 양자화 적용)
   * vLLM 사용 여부:
      - Transformers 라이브러리 사용
      - vLLM 라이브러리 사용

3. 평가 요소:
   * 모델 크기 (GB)
   * 추론 시간 (초)
   * 생성된 토큰 수
   * 초당 처리된 토큰 수 (tokens/second)




### 실험 방법

1. 실험 환경 준비:
   - IDE: GOOGLE COLAB
   - GPU 환경: A100

2. 실험 조건 설정:
   * 원본 모델 + Transformers
   * 원본 모델 + vLLM
   * 양자화 모델 + Transformers
   * 양자화 모델 + vLLM

3. 실험 절차 단계:
   * 모델 로드 및 초기화
   * 각 프롬프트에 대해 텍스트 생성 수행
   * 각 생성에 대한 추론 시간, 생성된 토큰 수, 초당 처리된 토큰 수 측정
   * 모델 크기 기록 (사용자가 직접 조사)
   * 결과의 평균값 계산




## 결과

### 추론 성능 비교

#### 추론 시간 (초)

| 모델 설정 | 프롬프트 1 | 프롬프트 2 | 프롬프트 3 |
|----------|-----------|-----------|-----------|
| 원본 모델 | 22.51 | 36.61 | 22.54 |
| 원본 모델 + vLLM | 10.23 | 7.69 | 9.39 |
| 양자화 모델 | 47.45 | 28.77 | 28.64 |
| 양자화 모델 + vLLM | **6.50** | **4.44** | **5.78** |

#### 처리 속도 (토큰/초)

| 모델 설정 | 프롬프트 1 | 프롬프트 2 | 프롬프트 3 |
|----------|-----------|-----------|-----------|
| 원본 모델 | 23.14 | 26.03 | 26.13 |
| 원본 모델 + vLLM | 68.49 | 68.94 | 68.83 |
| 양자화 모델 | 15.81 | 15.99 | 16.03 |
| 양자화 모델 + vLLM | **95.07** | **95.01** | **95.78** |

<img src="/assets/img/posts/20240927/추론성능비교그래프.png" alt="main" width="700" style="display: block; margin: auto;"/>

* 추론 시간:

모든 프롬프트에서 '양자화 모델 + vLLM' 설정이 가장 짧은 추론 시간을 보여줍니다.
'양자화 모델' 단독 사용 시 일부 프롬프트에서 가장 긴 추론 시간을 보입니다.
vLLM 사용 시 항상 추론 시간이 크게 감소했습니다.


* 처리 속도 (토큰/초):

'양자화 모델 + vLLM' 설정이 모든 프롬프트에서 가장 높은 처리 속도를 보여줍니다.
vLLM을 사용하지 않은 설정들은 상대적으로 낮은 처리 속도를 보입니다.
'양자화 모델' 단독 사용 시 가장 낮은 처리 속도를 보였습니다.


* 모델 간 차이:

vLLM을 사용한 모델들(원본 + vLLM, 양자화 + vLLM)이 사용하지 않은 모델들에 비해 월등히 좋은 성능을 보입니다.
양자화 모델은 단독으로 사용 시 성능이 저하되지만, vLLM과 결합하면 가장 좋은 성능을 보입니다.


* 프롬프트 간 차이:

프롬프트 간 차이는 있지만, 모델 설정 간 차이에 비하면 유의미하진 않습니다.
vLLM을 사용한 경우, 프롬프트 간 차이가 더욱 줄어듭니다.



### 하드웨어 사용량 비교

| 모델 설정 | 시스템 RAM (GB) | GPU RAM (GB) |
|----------|----------------|--------------|
| 원본 모델 | **3.3** | 15.8 |
| 원본 모델 + vLLM | 7.8 | 35.3* |
| 양자화 모델 | 3.4 | **6.0** |
| 양자화 모델 + vLLM | 7.7 | 35.7* |

\* 표시는 높은 메모리 사용량을 나타냅니다.
굵은 글씨는 각 카테고리에서 가장 낮은 사용량을 나타냅니다.


<img src="/assets/img/posts/20240927/하드웨어비교그래프.png" alt="main" width="700" style="display: block; margin: auto;"/>

* vLLM을 사용하는 설정(원본 모델 + vLLM, 양자화 모델 + vLLM)이 시스템 RAM과 GPU RAM 모두에서 높은 사용량을 보입니다. 양자화 모델이 단독으로 사용될 때 가장 낮은 GPU RAM 사용량을 보입니다.


* 원본 모델과 양자화 모델은 비슷한 수준의 시스템 RAM을 사용하지만, GPU RAM 사용량에서 큰 차이를 보입니다. vLLM 사용 시 하드웨어 리소스 사용량이 크게 증가하는 것을 명확히 볼 수 있습니다.


### 결론

추론 성능 자체는 양자화 + VLLM이 압도적이다. 하지만, 하드웨어 사용량도 압도적이다.. 
하드웨어 성능까지고려 해봤을 때, 그냥 원본 모델을 가져다 쓰는 것도 나쁘지 않은 선택이겠다 싶었습니다.
클라우드 환경에서 배포가 필수인 만큼, 비용과 성능의 TRADE OFF를 잘 따져가며 개발해야겠습니다. 