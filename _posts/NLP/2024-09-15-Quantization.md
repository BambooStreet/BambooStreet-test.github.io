---
title: "[NLP] LLM의 양자화란?"
author: BambooStreet
date: 2024-09-15 11:31:00 +0800
categories: []
tags: []
image: assets/img/posts/20240925/thumbnail.png
---

## 개요

최근 LLM의 모델의 크기는 기하 급수적으로 증가하고 있습니다.


단순한 예로, GPT-3는 1750억개의 파라미터를 가지고 있으며 이는 엄청난 양의 저장 공간과 메모리를 필요로 합니다.


이처럼 큰 모델은 추론 시 많은 계산 리소스를 필요로 하며, 이는 높은 운영 비용과 긴 처리 시간으로 이어저 사용자의 만족도를 떨어트리는 결과로 나타납니다.


특히 최근에는 챗봇과 같이 실시간 응답이 필요한 애플리케이션과 엣지 디바이스에서의 AI 활용이 트랜드가 되면서 제한된 메모리 안에서 빠른 추론 속도를 가지고, 모델을 구동할 필요가 있었고, 이를 위한 LLM 양자화 기술의 중요성이 부각되었습니다.




## 양자화란?

그렇다면 양자화란 무엇인지 알아보도록 합시다.


양자화의 기본 원리는 **연속적인 값**을 **이산적인 값**으로 매핑하는 것입니다.
LLM 컨텍스트에서 이는 주로 부동소수점 값을 정수값으로 변환하는 과정을 의미합니다.


일반적인 LLM에서 가중치(weight)와 각 노드의 활성화(activation) 출력값은 32비트 부동소수점 형식으로 저장됩니다. 이 형식은 넓은 범위의 값을 높은 정밀도로 표현할 수 있지만, 많은 저장 공간과 계산 리소스를 필요로 합니다.


양자화를 진행하면서, 우선 모델의 가중치와 활성화 값의 분포를 분석합니다. 분석과정은 다음과 같습니다. 


1. 각 층의 가중치와 출력 값의 최솟값, 최대값, 평균, 표준편차 등을 계산합니다.
2. 부동소수점 값을 정수로 변환할 때, 사용할 스케일 팩터를 결정합니다. 이 스케일은 양자화의 정밀도와 표현 가능한 범위에 직접적인 영향을 미치게 됩니다.



또 분석된 범위를 바탕으로 양자화 스케일을 결정합니다. 








물론이죠. LLM 양자화의 원리에 대해 더 자세히 설명해드리겠습니다.

양자화의 기본 원리는 연속적인 값을 이산적인 값으로 매핑하는 것입니다. LLM 컨텍스트에서 이는 주로 부동소수점 값을 정수값으로 변환하는 과정을 의미합니다. 

1. 부동소수점 표현:
   일반적으로 LLM의 가중치와 활성화는 32비트 부동소수점 형식으로 저장됩니다. 이 형식은 넓은 범위의 값을 높은 정밀도로 표현할 수 있지만, 많은 저장 공간과 계산 리소스를 필요로 합니다.

2. 양자화 과정:
   a) 값 범위 분석: 모델의 가중치와 활성화 값의 분포를 분석합니다.
   b) 스케일 결정: 분석된 범위를 바탕으로 양자화 스케일을 결정합니다.
   c) 매핑: 부동소수점 값을 정수값으로 매핑합니다.

3. 선형 양자화:
   가장 흔한 양자화 방식 중 하나로, 다음과 같은 공식을 사용합니다:
   q = round(x / scale + zero_point)
   여기서 x는 원본 부동소수점 값, q는 양자화된 정수값, scale은 스케일 팩터, zero_point는 0을 표현하는 정수값입니다.

4. 역양자화:
   양자화된 값을 다시 부동소수점으로 변환할 때는 다음 공식을 사용합니다:
   x = (q - zero_point) * scale

5. 비대칭 양자화:
   음수와 양수의 범위가 다를 때 사용됩니다. 이 경우 양수와 음수에 대해 서로 다른 스케일을 적용할 수 있습니다.

6. 가중치-활성화 양자화:
   가중치와 활성화에 대해 서로 다른 양자화 방식을 적용할 수 있습니다. 예를 들어, 가중치는 8비트로, 활성화는 4비트로 양자화할 수 있습니다.

7. 양자화 인식 학습(QAT):
   학습 과정에서 양자화의 효과를 시뮬레이션하여 양자화로 인한 정확도 손실을 최소화합니다. 이는 '가짜' 양자화 연산을 삽입하여 수행됩니다.

8. 혼합 정밀도 양자화:
   모델의 일부 레이어는 높은 정밀도(예: 16비트)로, 다른 레이어는 낮은 정밀도(예: 4비트)로 양자화합니다. 이는 중요한 레이어의 정확도를 유지하면서 전체적인 모델 크기를 줄이는 데 효과적입니다.

9. 벡터 양자화:
   개별 값이 아닌 벡터 단위로 양자화를 수행합니다. 이는 특히 행렬 곱셈 연산이 많은 LLM에서 효과적일 수 있습니다.

10. 오차 보정:
    양자화 과정에서 발생하는 오차를 다음 레이어나 연산에 전파하여 보정합니다. 이를 통해 누적 오차를 줄일 수 있습니다.

이러한 원리들을 적절히 조합하고 적용함으로써, LLM의 크기를 크게 줄이고 추론 속도를 향상시키면서도 정확도 손실을 최소화할 수 있습니다. 양자화 기술은 계속 발전하고 있으며, 더욱 효율적이고 정확한 방법들이 연구되고 있습니다.





