---
title: "[HCI] AI의 윤리적 문제(Ethical issues)"
author: BambooStreet
date: 2024-10-22 00:00:00 +0800
categories: []
tags: []
image: assets/img/posts/20241021/Inside_out.png
---

## 개요

<div style="text-align: center;">
    <img src="/assets/img/posts/20241021/Inside_out.png" alt="main" width="300" style="display: block; margin: auto;"/>
    <em>감정을 의인화한 Inside Out</em>
</div>


구글 태그 인공지능이 고릴라와 흑인을 구분하지 못했던 사건,

국내 이루다 서비스, 필터링

아마존 인공지능 채용 성차별 사건,

세계 각국의 윤리적 문제와 관련된 인공지능 법제도 마련 중

* 인공지능의 편향을 발생시킬 수 있는 데이터의 속성은 어디까지 판별 가능한가?
* 윤리적 문제가 될 수 있을만한 부분을 5:5 또는 Evenly distributed 하게 처리하는 것이 항상 옳은 것인가? (어쩌면 도덕주의적 오류(Moralistic fallacy)로 비춰질 수도 있음)
* 우리가 미처 파악하지 못한 부분에서 발생한 특징적 요소로 인해 나타나는 윤리적 문제는 어떻게 대처할 것인가? (의사가 반영되지 않은 피해에 대해 책임을 물을 수 있는가?)


## 인공지능, 데이터 윤리 문제

인공지능 기술은 분명 사회에 주는 이익이 있음 반대로 악용될 여지도 다분하다.


하지만, 모든 윤리적 기준을 충족하고, 모든 동의를 받아 데이터를 수집하면 상용화는 물론 개발 경쟁에서 뒤처질 수 있음


테슬라, 애플 페이스북 갈등, 구글 클라우드 등


결국 인공지능 기업이 가장 중요하게 생각하고 크게 의존할 수 밖에 없는 것은 데이터, 


## 문제 해결을 위한 연구

Stahl, B. C., & Stahl, B. C. (2021). Ethical issues of AI. Artificial Intelligence for a better future: An ecosystem perspective on the ethics of AI and emerging digital technologies,


AI 관계자들이 인식하는 윤리적 문제를 식별하기 위해 10개의 사례 연구와 델파이 연구를 수행, 기계 학습, 디지털 세계, 형이상학적 문제로 분류된 39개의 윤리적 문제 식별

#### 기계학습에서의 윤리적 문제

* 프라이버시 및 데이터 보호: AI의 대규모 데이터셋 필요성은 정보 프라이버시와 데이터 보안에 대한 위험을 초래한다.
* 신뢰성: 기계학습 시스템의 불투명성과 예측 불가능성은 전통적인 신뢰성 평가에 도전한다.
* 투명성과 책임성: 기계 학습 알고리즘의 투명성 부족은 책임 문제를 야기한다.
* 편향과 차별: 기계 학습은 기존의 편향을 재생산하고, 차별을 초래할 수 있다.
* 안전성: 자율 주행 차량과 같이 물리적 세계와 상호작용하는 AI 시스템은 안전 문제를 야기한다.


#### 디지털 세계에서의 일반적 문제

* 경제적 영향: AI는 일자리 대체를 초래할 수 있으며, 이는 소수의 대형 기술 회사에 경제적 및 정치적 권력을 집중시킨다.
* 정의와 공정성: AI는 기존의 불평등을 악화시키고 사법 접근 및 공공 서비스 접근에 영향을 미칠 수 있다.
* 자유와 자율성: AI는 행동과 의사 결정 옵션을 형성해 개인의 자율성을 제한할 수 있다.
* 더 넓은 사회적 문제: AI의 민주주의, 권력 관계, 환경에 대한 영향은 중요한 윤리적 문제를 야기한다.

#### 형이상학적(Metaphisical) 문제

* 인공지능 일반 지능(Artificial general intelligence, AGI): AGI에 대한 우려는 책임, 의식, AI의 윤리적 지위에 대한 질문을 제시한다.
* 실존적 질문: 초지능과 특이점에 대한 논쟁은 인간의 본성과 윤리적 주체성에 대한 근본적인 질문을 제기한다.



## 조직 입장에서 AI 윤리 문제 대응

Stahl, B. C., Antoniou, J., Ryan, M., Macnish, K., & Jiya, T. (2022). Organisational responses to the ethical issues of artificial intelligence. AI & SOCIETY, 37(1), 23-37.


조직의 입장에서 AI의 윤리적 문제와 대응에 대해 조사한 실증 연구 논문입니다. 10개의 사례 연구를 통해 조직들이 실제로 어떤 대응을 하는지 분석


#### 윤리적 문제 해결을 위한 완화(Mitigation) 전략

1. 정책 수준 대응: 데이터 보호법, 책임성, 지적 재산권 법률 등이 포함되며, 규제 기관의 역할과 새로운 규제 기관의 필요성 논의도 포함
2. 지침 메커니즘: AI 윤리 지침, 전문가 단체의 행동 규범, 표준화 및 인증 등이 포함, 다양한 도구와 방법론을 사용하여 실질적인 지침을 제공할 수 있음
3. 기업 거버넌스 메커니즘: 정보 거버넌스와 데이터 거버넌스는 윤리적 문제 해결에 중요한 역할, 위험 관리 전략과 인권, 윤리적 문제에 대한 평가가 이뤄질 수 있음

#### 조직의 AI 윤리 문제 해결 주요 전략

1. 조직의 인식 및 반성
2. 기술적 접근
3. 인간의 감독
4. 윤리 교육 및 훈련
5. 경쟁하는 가치의 균형



## AI 에이전트가 인간의 윤리적 행동에 미치는 영향

Köbis, N., Bonnefon, J. F., & Rahwan, I. (2021). Bad machines corrupt good morals. Nature human behaviour, 5(6), 679-685.


이 연구는 행동과학, 인간-컴퓨터 상호작용, AI 관련 문헌을 검토하여 AI 에이전트가 인간의 윤리적 행동에 어떻게 영향을 미치는지 알아본다. 문헌 검토를 통해 AI 에이전트는 아래와 같이 크게 네가지 유형으로 인간의 윤리적 행동에 영향을 미칠 수 있다고 보고있다.


#### 4가지 영향 유형

<간접적 영향(Influencer)>
* 조언자(Advisor)
  * AI 에이전트는 사람이 비윤리적 행동을 하도록 유도
  * AI의 조언은 인간의 조언과 유사한 부정적 효과를 가져올 수 있다.
* 롤 모델(Role model)
  * AI 에이전트는 행동의 예를 설정해 사람들에게 영향을 미칠 수 있다.
  * 과거 연구들을 보면 AI의 영향력은 인간 롤 모델보다 크지 않다고 보여지지만, 아이들의 경우는 다를 수 있다.

<직접적 조력(Enabler)>
* 파트너(Partner)
  * AI 에이전트는 비윤리적 행동의 협력자로 활동 가능
  * 사람들은 자신의 행동 정당화, AI에 책임 전가
* 대리인(Delegate)
  * 비윤리적 작업을 AI에 위임 가능, 익명성 제공 및 죄책감 경감
  * 위임은 비윤리적 행동을 매력적으로 만들고, 인간이 시작한 것으로 추적하기 어렵게 만든다.

